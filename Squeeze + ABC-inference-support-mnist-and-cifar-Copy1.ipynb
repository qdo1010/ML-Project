{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Accurate Binary Convolution Layer\n",
    "The main notebook is **ABC.ipynb**. In this notebook, *alphas* training is moved out of the layer, so that the variables and functions can be made reusable for inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10,mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See *ABC* notebook for explanation of all the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_stddev(input_tensor):\n",
    "    with tf.name_scope('mean_stddev_cal'):\n",
    "        mean, variance = tf.nn.moments(input_tensor, axes=range(len(input_tensor.get_shape())))\n",
    "        stddev = tf.sqrt(variance, name=\"standard_deviation\")\n",
    "        return mean, stddev\n",
    "    \n",
    "# TODO: Allow shift parameters to be learnable\n",
    "def get_shifted_stddev(stddev, no_filters):\n",
    "    with tf.name_scope('shifted_stddev'):\n",
    "        spreaded_deviation = -1. + (2./(no_filters - 1)) * tf.convert_to_tensor(range(no_filters),\n",
    "                                                                                dtype=tf.float32)\n",
    "        return spreaded_deviation * stddev\n",
    "    \n",
    "def get_binary_filters(convolution_filters, no_filters, name=None):\n",
    "    with tf.name_scope(name, default_name=\"get_binary_filters\"):\n",
    "        mean, stddev = get_mean_stddev(convolution_filters)\n",
    "        shifted_stddev = get_shifted_stddev(stddev, no_filters)\n",
    "        \n",
    "        # Normalize the filters by subtracting mean from them\n",
    "        mean_adjusted_filters = convolution_filters - mean\n",
    "        \n",
    "        # Tiling filters to match the number of filters\n",
    "        expanded_filters = tf.expand_dims(mean_adjusted_filters, axis=0, name=\"expanded_filters\")\n",
    "        tiled_filters = tf.tile(expanded_filters, [no_filters] + [1] * len(convolution_filters.get_shape()),\n",
    "                                name=\"tiled_filters\")\n",
    "        \n",
    "        # Similarly tiling spreaded stddev to match the shape of tiled_filters\n",
    "        expanded_stddev = tf.reshape(shifted_stddev, [no_filters] + [1] * len(convolution_filters.get_shape()),\n",
    "                                     name=\"expanded_stddev\")\n",
    "        \n",
    "        binarized_filters = tf.sign(tiled_filters + expanded_stddev, name=\"binarized_filters\")\n",
    "        return binarized_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, instead of get_alphas, implementation of **alpha training** is provided, which takes input of the *filters*, *binarized filters*, and *alphas* and returns the loss and the alpha training operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_training(convolution_filters, binary_filters, alphas, no_filters):\n",
    "    with tf.name_scope(\"alpha_training\"):\n",
    "        reshaped_convolution_filters = tf.reshape(convolution_filters, [-1], name=\"reshaped_convolution_filters\")\n",
    "        reshaped_binary_filters = tf.reshape(binary_filters, [no_filters, -1],\n",
    "                                             name=\"reshaped_binary_filters\")\n",
    "        \n",
    "        weighted_sum_filters = tf.reduce_sum(tf.multiply(alphas, reshaped_binary_filters),\n",
    "                                             axis=0, name=\"weighted_sum_filters\")\n",
    "        \n",
    "        # Defining loss\n",
    "        error = tf.square(reshaped_convolution_filters - weighted_sum_filters, name=\"alphas_error\")\n",
    "        loss = tf.reduce_mean(error, axis=0, name=\"alphas_loss\")\n",
    "        \n",
    "        # Defining optimizer\n",
    "        training_op = tf.train.AdamOptimizer().minimize(loss, var_list=[alphas],\n",
    "                                                        name=\"alphas_training_op\")\n",
    "        \n",
    "        return training_op, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, both *ABC* and *ApproxConv* is updated to incorporate this change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApproxConv(no_filters, alphas, binary_filters, convolution_biases=None,\n",
    "               strides=(1, 1), padding=\"VALID\", name=None):\n",
    "    with tf.name_scope(name, \"ApproxConv\"):\n",
    "        if convolution_biases is None:\n",
    "            biases = 0.\n",
    "        else:\n",
    "            biases = convolution_biases\n",
    "        \n",
    "        # Defining function for closure to accept multiple inputs with same filters\n",
    "        def ApproxConvLayer(input_tensor, name=None):\n",
    "            with tf.name_scope(name, \"ApproxConv_Layer\"):\n",
    "                # Reshaping alphas to match the input tensor\n",
    "                reshaped_alphas = tf.reshape(alphas,\n",
    "                                             shape=[no_filters] + [1] * len(input_tensor.get_shape()),\n",
    "                                             name=\"reshaped_alphas\")\n",
    "                \n",
    "                # Calculating convolution for each binary filter\n",
    "                approxConv_outputs = []\n",
    "                for index in range(no_filters):\n",
    "                    # Binary convolution\n",
    "                    this_conv = tf.nn.conv2d(input_tensor, binary_filters[index],\n",
    "                                             strides=(1,) + strides + (1,),\n",
    "                                             padding=padding)\n",
    "                    approxConv_outputs.append(this_conv + biases)\n",
    "                conv_outputs = tf.convert_to_tensor(approxConv_outputs, dtype=tf.float32,\n",
    "                                                    name=\"conv_outputs\")\n",
    "                \n",
    "                # Summing up each of the binary convolution\n",
    "                ApproxConv_output = tf.reduce_sum(tf.multiply(conv_outputs, reshaped_alphas), axis=0)\n",
    "                \n",
    "                return ApproxConv_output\n",
    "        \n",
    "        return ApproxConvLayer\n",
    "    \n",
    "def ABC(binary_filters, alphas, shift_parameters, betas, \n",
    "        convolution_biases=None, no_binary_filters=2, no_ApproxConvLayers=1,\n",
    "        strides=(1, 1), padding=\"VALID\", name=None):\n",
    "    with tf.name_scope(name, \"ABC\"):        \n",
    "        # Instantiating the ApproxConv Layer\n",
    "        ApproxConvLayer= ApproxConv(no_binary_filters, alphas, binary_filters, convolution_biases,\n",
    "                                    strides, padding)\n",
    "        \n",
    "        def ABCLayer(input_tensor, name=None):\n",
    "            with tf.name_scope(name, \"ABCLayer\"):\n",
    "                # Reshaping betas to match the input tensor\n",
    "                reshaped_betas = tf.reshape(betas,\n",
    "                                            shape=[no_ApproxConvLayers] + [1] * len(input_tensor.get_shape()),\n",
    "                                            name=\"reshaped_betas\")\n",
    "                \n",
    "                # Calculating ApproxConv for each shifted input\n",
    "                ApproxConv_layers = []\n",
    "                for index in range(no_ApproxConvLayers):\n",
    "                    # Shifting and binarizing input\n",
    "                    shifted_input = tf.clip_by_value(input_tensor + shift_parameters[index], 0., 1.,\n",
    "                                                     name=\"shifted_input_\" + str(index))\n",
    "                    binarized_activation = tf.sign(shifted_input - 0.5)\n",
    "                    \n",
    "                    # Passing through the ApproxConv layer\n",
    "                    ApproxConv_layers.append(ApproxConvLayer(binarized_activation))\n",
    "                ApproxConv_output = tf.convert_to_tensor(ApproxConv_layers, dtype=tf.float32,\n",
    "                                                         name=\"ApproxConv_output\")\n",
    "                \n",
    "                # Taking the weighted sum using the betas\n",
    "                ABC_output = tf.reduce_sum(tf.multiply(ApproxConv_output, reshaped_betas), axis=0)\n",
    "                return ABC_output\n",
    "        \n",
    "        return ABCLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now a layer can be created as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filters = np.random.normal(size=(3, 3, 1, 64))\n",
    "test_biases = np.random.normal(size=(64,))\n",
    "test_input = np.random.normal(size=(32, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot reshape a tensor with 5 elements to shape [1,1,1,1,1] (1 elements) for 'ABCLayer/reshaped_betas' (op: 'Reshape') with input shapes: [5,1], [5] and with input tensors computed as partial shapes: input[1] = [1,1,1,1,1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-c3c2d1262d6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mABC_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mABC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mABC_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-163-e6cfe697fb19>\u001b[0m in \u001b[0;36mABCLayer\u001b[0;34m(input_tensor, name)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 reshaped_betas = tf.reshape(betas,\n\u001b[1;32m     47\u001b[0m                                             \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mno_ApproxConvLayers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                                             name=\"reshaped_betas\")\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;31m# Calculating ApproxConv for each shifted input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   5778\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5779\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 5780\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   5781\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5782\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3338\u001b[0m       \u001b[0;31m# is removed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3339\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_USE_C_SHAPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3340\u001b[0;31m         \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3342\u001b[0m       self._create_op_helper(ret, compute_shapes=compute_shapes,\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2530\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs_c_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2531\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2532\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_set_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2503\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2505\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2506\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2426\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2427\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot reshape a tensor with 5 elements to shape [1,1,1,1,1] (1 elements) for 'ABCLayer/reshaped_betas' (op: 'Reshape') with input shapes: [5,1], [5] and with input tensors computed as partial shapes: input[1] = [1,1,1,1,1]."
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    filters = tf.Variable(tf.convert_to_tensor(test_filters, dtype=tf.float32), name=\"convolution_filters\")\n",
    "    biases = tf.Variable(tf.convert_to_tensor(test_biases, dtype=tf.float32), name=\"convolution_biases\")\n",
    "    alphas = tf.Variable(tf.constant(1., shape=(5, 1)), dtype=tf.float32,\n",
    "                         name=\"alphas\")\n",
    "    shift_parameters = tf.Variable(tf.constant(0., shape=(5, 1)), dtype=tf.float32,\n",
    "                                   name=\"shift_parameters\")\n",
    "    betas = tf.Variable(tf.constant(1., shape=(5, 1)), dtype=tf.float32,\n",
    "                        name=\"betas\")\n",
    "    \n",
    "    binary_filters = get_binary_filters(filters, 5)\n",
    "    alphas_training_op, alphas_loss = alpha_training(tf.stop_gradient(filters),\n",
    "                                                     tf.stop_gradient(binary_filters),\n",
    "                                                     alphas, 5)\n",
    "    ABC_layer = ABC(binary_filters, tf.stop_gradient(alphas), shift_parameters, betas, biases)\n",
    "    \n",
    "    output = ABC_layer(tf.convert_to_tensor(test_input, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Let's test the updated architecture on cifar again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 1)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "def next_experiment_dir(top_dir):\n",
    "    \"\"\"We need directory with consecutive subdirectories to store results of consecutive trainings. \"\"\"\n",
    "    dirs = [int(dirname) for dirname in os.listdir(top_dir) if os.path.isdir(os.path.join(top_dir, dirname))]\n",
    "    if len(dirs) > 0:\n",
    "        return os.path.join(top_dir, str(max(dirs) + 1))\n",
    "    else:\n",
    "        return os.path.join(top_dir, '1')\n",
    "\n",
    "\n",
    "def prepare_input(data, mu=None, sigma=None):\n",
    "    \"\"\"\n",
    "    Normalizes pixels across dataset. For training set, calculate mu and sigma. For test set, transfer these\n",
    "    from training set.\n",
    "\n",
    "    :param data: dataset\n",
    "    :param mu: mean pixel value across dataset. Calculated if not provided.\n",
    "    :param sigma: standard deviation of pixel value across dataset. Calculated if not provided.\n",
    "    :return: normalized dataset, mean and standard deviation\n",
    "    \"\"\"\n",
    "    if mu is None:\n",
    "        mu = np.mean(data)\n",
    "    if sigma is None:\n",
    "        sigma = np.std(data)\n",
    "    data = data - mu\n",
    "    data = data / sigma\n",
    "    return data, mu, sigma\n",
    "\n",
    "image_size = 32\n",
    "num_labels = 10\n",
    "num_channels = 3 # RGB\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    " \n",
    "x_train, mu_train, sigma_train = prepare_input(x_train)\n",
    "x_test, _, _ = prepare_input(x_test, mu_train, sigma_train)\n",
    "\n",
    "#this part is needed to reshape mnist. Cifar doesn't matter. just comment it out if cifar\n",
    "#x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "#x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "#y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "#y_test = y_test.reshape(y_test.shape[0], 1)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 1)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "import cv2\n",
    "import random\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import uuid\n",
    "\n",
    "#initial_epoch = 0\n",
    "#nb_epoch = 10\n",
    "#batch_size = 64\n",
    "#validation_split = 0.2 \n",
    "#input_shape = (67, 67, 3)\n",
    "\n",
    "#nb_classes = 10\n",
    "\n",
    "\n",
    "def load_image(img):\n",
    "    # Load image with 3 channel colors\n",
    "    # img = cv2.imread(img_path, flags=1)\n",
    "    name = str(uuid.uuid4())\n",
    "\n",
    "    # Convert to rgb\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    # cv2.imwrite('/tmp/test/%s.jpg' % name, img)\n",
    "    # Image needs to the resized to (227x227), but we want to maintain the aspect ratio.\n",
    "    # height = img.shape[0]\n",
    "    # width = img.shape[1]\n",
    "    # offset = int(round(max(height, width) / 2.0))\n",
    "\n",
    "    # # Add borders to the images.\n",
    "    # padded_img = cv2.copyMakeBorder(img, offset, offset, offset, offset, cv2.BORDER_CONSTANT)\n",
    "    # padded_height = padded_img.shape[0]\n",
    "    # padded_width = padded_img.shape[1]\n",
    "    # center_x = int(round(padded_width / 2.0))\n",
    "    # center_y = int(round(padded_height / 2.0))\n",
    "    # # Crop the square containing the full image.\n",
    "    # cropped_img = padded_img[center_y - offset: center_y + offset, center_x - offset: center_x + offset]\n",
    "\n",
    "    # Resize image to 227, 227 as Squeezenet only accepts this format.\n",
    "    resized_image = cv2.resize(img, (input_shape[0], input_shape[1])).astype('float32')\n",
    "    return resized_image\n",
    "\n",
    "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "## Shuffle lists\n",
    "#train_zipped = zip(X_train, y_train)\n",
    "#test_zipped = zip(X_test, y_test)\n",
    "\n",
    "\n",
    "#random.shuffle(train_zipped)\n",
    "#random.shuffle(train_zipped)\n",
    "#random.shuffle(train_zipped)\n",
    "#random.shuffle(test_zipped)\n",
    "#random.shuffle(test_zipped)\n",
    "#random.shuffle(test_zipped)\n",
    "\n",
    "#X_train[:], y_train[:] = zip(*train_zipped)\n",
    "#X_test[:], y_test[:] = zip(*test_zipped)\n",
    "\n",
    "#X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "#X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "#X_train = X_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "#X_train /= 255\n",
    "#X_test /= 255\n",
    "#Y_train = to_categorical(y_train, nb_classes)\n",
    "#Y_test = to_categorical(y_test, nb_classes)\n",
    "\n",
    "def gen(x, y, nb=50): \n",
    "    for i in xrange(len(x)):\n",
    "        j = random.randint(i, len(x)-2000)\n",
    "        newy = y[j: j+nb]\n",
    "        newx = x[j:j+nb]\n",
    "        #newx = np.asarray([load_image(img) for img in x[j:j+nb]])\n",
    "        return (newx,newy)\n",
    "        #yield (np.asarray([load_image(img) for img in x[j:j+nb]]),y[j: j+nb])\n",
    "        # yield (np.asarray([load_image(img) for img in x[j*nb:j*nb+nb]]), y[j*nb:j*nb+nb])\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "# train, test = mnist.load_data()\n",
    "# train = zip(*train)\n",
    "# test = zip(*test)\n",
    "\n",
    "#samples_per_epoch = 3000 #len(training_images) - 20\n",
    "#nb_val_samples = 300 # len(validation_images) - 20\n",
    "\n",
    "# Generator expression. Yields two tuples (image, class). Use generator because images might not fit into memory,\n",
    "# training_data = ( (load_image(x), to_categorical([y], nb_classes=nb_classes)) for x, y in train )\n",
    "# validation_data = ( (load_image(x), to_categorical([y], nb_classes=nb_classes)) for x, y in test )\n",
    "# training_data = gen(X_train, Y_train)\n",
    "# validation_data = gen(X_test, Y_test)\n",
    "\n",
    "#x_train,y_train = gen(x_train, y_train, nb=6000) #change this to get bigger dataset\n",
    "#x_test,y_test = gen(x_test, y_test, nb=1000) #change this to get more validation set\n",
    "\n",
    "\n",
    "#print (X_train.shape)\n",
    "#print (Y_test.shape)\n",
    "\n",
    "num_images_train = x_train.shape[0]\n",
    "num_images_valid = x_test.shape[0]\n",
    "\n",
    "print (x_train.shape)\n",
    "print (y_train.shape)\n",
    "print (x_test.shape)\n",
    "print (y_test.shape)\n",
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is exactly same as in the other notebook *ABC*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining utils function\n",
    "def weight_variable(shape, name=\"weight\"):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name=\"bias\"):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def squeeze(input, channels, layer_num):\n",
    "    \"\"\"\n",
    "    Defines squeezed block for fire module.\n",
    "\n",
    "    :param input: input tensor\n",
    "    :param channels: number of output channels\n",
    "    :param layer_num: layer number for naming purposes\n",
    "    :return: output tensor convoluted with squeeze layer\n",
    "    \"\"\"\n",
    "    layer_name = 'squeeze_' + str(layer_num)\n",
    "    input_channels = input.get_shape().as_list()[3]\n",
    "\n",
    "    with tf.name_scope(layer_name):\n",
    "        weights = tf.Variable(tf.contrib.layers.xavier_initializer()([1, 1, input_channels, channels]))\n",
    "        biases = tf.Variable(tf.zeros([1, 1, 1, channels]), name='biases')\n",
    "        onebyone = tf.nn.conv2d(input, weights, strides=(1, 1, 1, 1), padding='VALID') + biases\n",
    "        A = tf.nn.relu(onebyone)\n",
    "\n",
    "        tf.summary.histogram('weights', weights)\n",
    "        tf.summary.histogram('biases', biases)\n",
    "        tf.summary.histogram('logits', onebyone)\n",
    "        tf.summary.histogram('activations', A)\n",
    "\n",
    "    return A\n",
    "\n",
    "# define expand module\n",
    "def expand(input, channels_1by1, channels_3by3, layer_num):\n",
    "    \"\"\"\n",
    "    Defines expand block for fire module.\n",
    "    :param input: input tensor\n",
    "    :param channels_1by1: number of output channels in 1x1 layers\n",
    "    :param channels_3by3: number of output channels in 3x3 layers\n",
    "    :param layer_num: layer number for naming purposes\n",
    "    :return: output tensor convoluted with expand layer\n",
    "    \"\"\"\n",
    "\n",
    "    layer_name = 'expand_' + str(layer_num)\n",
    "    input_channels = input.get_shape().as_list()[3]\n",
    "\n",
    "    with tf.name_scope(layer_name):\n",
    "        weights1x1 = tf.Variable(tf.contrib.layers.xavier_initializer()([1, 1, input_channels, channels_1by1]))\n",
    "        biases1x1 = tf.Variable(tf.zeros([1, 1, 1, channels_1by1]), name='biases')\n",
    "        onebyone = tf.nn.conv2d(input, weights1x1, strides=(1, 1, 1, 1), padding='VALID') + biases1x1\n",
    "        A_1x1 = tf.nn.relu(onebyone)\n",
    "\n",
    "        tf.summary.histogram('weights_1x1', weights1x1)\n",
    "        tf.summary.histogram('biases_1x1', biases1x1)\n",
    "        tf.summary.histogram('logits_1x1', onebyone)\n",
    "        tf.summary.histogram('activations_1x1', A_1x1)\n",
    "\n",
    "        weights3x3 = tf.Variable(tf.contrib.layers.xavier_initializer()([1, 1, input_channels, channels_3by3]))\n",
    "        biases3x3 = tf.Variable(tf.zeros([1, 1, 1, channels_3by3]), name='biases')\n",
    "        threebythree = tf.nn.conv2d(input, weights3x3, strides=(1, 1, 1, 1), padding='SAME') + biases3x3\n",
    "        A_3x3 = tf.nn.relu(threebythree)\n",
    "\n",
    "        tf.summary.histogram('weights_3x3', weights3x3)\n",
    "        tf.summary.histogram('biases_3x3', biases3x3)\n",
    "        tf.summary.histogram('logits_3x3', threebythree)\n",
    "        tf.summary.histogram('activations_3x3', A_3x3)\n",
    "\n",
    "    return tf.concat([A_1x1, A_3x3], axis=3)\n",
    "\n",
    "\n",
    "# define fire module\n",
    "def fire_module(input, squeeze_channels, expand_channels_1by1, expand_channels_3by3, layer_num):\n",
    "    \"\"\"\n",
    "    Train fire module. Fire module does not change input height and width, only depth.\n",
    "    :param input: input tensor\n",
    "    :param squeeze_channels: number of channels for 1x1 squeeze layer\n",
    "    :param expand_channels_1by1: number of channels for 1x1 expand layer\n",
    "    :param expand_channels_3by3: number of channels for 3x3 expand layer\n",
    "    :param layer_num: number of layer for naming purposes only\n",
    "    :return: a tensor of shape [input_height x input_width x expand_channels_1by1 * expand_channels_3by3]\n",
    "    \"\"\"\n",
    "    with tf.name_scope('fire_' + str(layer_num)):\n",
    "        squeeze_output = squeeze(input, squeeze_channels, layer_num)\n",
    "        return expand(squeeze_output, expand_channels_1by1, expand_channels_3by3, layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create graph\n",
    "\n",
    "#filters = [64,64,128,128,192,192,256,256]\n",
    "#squeezes= [16,16, 32, 32, 48, 48, 64, 64]\n",
    "\n",
    "#filters = [8,8,16,16,24,24,32,32]\n",
    "#squeezes= [2,2,4, 4, 6, 6, 8, 8]\n",
    "without_ABC_graph = tf.Graph()\n",
    "with without_ABC_graph.as_default():\n",
    "    # define placeholders\n",
    "        pooling_size=(1, 2, 2, 1)\n",
    "        x = tf.placeholder(tf.float32,\n",
    "                                     shape=[None, 28, 28, 1],\n",
    "                                     name='x')\n",
    "        x_image = x\n",
    "\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        y = tf.placeholder(tf.int32, [None,1],name='y')\n",
    "        in_training = tf.placeholder(tf.bool, shape=())\n",
    "        learning_rate = tf.placeholder(tf.float32, shape=())\n",
    "\n",
    "        tf.summary.image('x', x)\n",
    "    # define structure of the net\n",
    "    # layer 1 - conv 1\n",
    "        with tf.name_scope('conv_1'):\n",
    "            W_conv1 = tf.Variable(tf.contrib.layers.xavier_initializer()([5, 5, 1, 96]),name='W_conv1')\n",
    "            b_conv1 = tf.Variable(tf.zeros([1, 1, 1, 96]),name='b_conv1')\n",
    "            X_1 = tf.nn.conv2d(x_image, W_conv1, strides=(1, 2, 2, 1), padding='VALID') + b_conv1\n",
    "            A_1 = tf.nn.relu(X_1)\n",
    "            tf.summary.histogram('conv1_weights', W_conv1)\n",
    "            tf.summary.histogram('conv1_biases', b_conv1)\n",
    "            tf.summary.histogram('conv1_logits', X_1)\n",
    "            tf.summary.histogram('conv1_activations', A_1)\n",
    "        #maxpool2\n",
    "        maxpool_1 = tf.nn.max_pool(A_1, ksize=pooling_size, strides=(1, 2, 2, 1), padding='VALID', name='maxpool_1')\n",
    "\n",
    "        # layer 3-5 - fire modules\n",
    "        fire_2 = fire_module(maxpool_1, 16, 64, 64, layer_num=2)\n",
    "        fire_3 = fire_module(fire_2, 16, 64, 64, layer_num=3)\n",
    "        fire_4 = fire_module(fire_3, 32, 128, 128, layer_num=4)\n",
    "\n",
    "        # layer 6 - maxpool\n",
    "        maxpool_4 = tf.nn.max_pool(fire_4, ksize=pooling_size, strides=(1, 2, 2, 1), padding='VALID', name='maxpool_4')\n",
    "\n",
    "        # layer 7-10 - fire modules\n",
    "        fire_5 = fire_module(maxpool_4, 32, 128, 128, layer_num=5)\n",
    "        fire_6 = fire_module(fire_5, 48, 192, 192, layer_num=6)\n",
    "        fire_7 = fire_module(fire_6, 48, 192, 192, layer_num=7)\n",
    "        fire_8 = fire_module(fire_7, 64, 256, 256, layer_num=8)\n",
    "\n",
    "        # layer 11 - maxpool\n",
    "        maxpool_8 = tf.nn.max_pool(fire_8, ksize=pooling_size, strides=(1, 2, 2, 1), padding='VALID', name='maxpool_8')\n",
    "\n",
    "        # layer 12 - fire 9 + dropout\n",
    "        fire_9 = fire_module(maxpool_8, 64, 256, 256, layer_num=9)\n",
    "\n",
    "        dropout_9 = tf.cond(in_training,\n",
    "                            lambda: tf.nn.dropout(fire_9, keep_prob=0.5),\n",
    "                            lambda: fire_9)\n",
    "\n",
    "        input_shape = dropout_9.get_shape().as_list()[3]\n",
    "        # layer 13 - final\n",
    "        with tf.name_scope('final'):\n",
    "            W_conv10 = tf.Variable(tf.contrib.layers.xavier_initializer()([1, 1, input_shape, 10]),name=\"W_conv10\")\n",
    "            b_conv10 = tf.Variable(tf.zeros([1, 1, 1, 10]),name=\"b_conv10\")\n",
    "            conv_10 = tf.nn.conv2d(dropout_9, W_conv10, strides=(1, 1, 1,1), padding='VALID') + b_conv10\n",
    "            A_conv_10 = tf.nn.relu(conv_10)\n",
    "\n",
    "            tf.summary.histogram('conv10_weights', W_conv10)\n",
    "            tf.summary.histogram('conv10_biases', b_conv10)\n",
    "            tf.summary.histogram('conv10_logits', conv_10)\n",
    "            tf.summary.histogram('conv10_activations', A_conv_10)\n",
    "        \n",
    "        # avg pooling to get [1 x 1 x num_classes] must average over entire window oh H x W from input layer\n",
    "        _, H_last, W_last, _ = A_conv_10.get_shape().as_list()\n",
    "        pooled = tf.nn.avg_pool(A_conv_10, ksize=(1, H_last, W_last, 1), strides=(1, 1, 1, 1), padding='VALID')\n",
    "        logits = tf.squeeze(pooled, axis=[1, 2])\n",
    "\n",
    "        # loss + optimizer\n",
    "        #one_hot_labels = y   \n",
    "        one_hot_labels = tf.one_hot(y, 10, name='one_hot_encoding')\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot_labels, logits=logits))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # accuracy\n",
    "        predictions = tf.reshape(tf.argmax(tf.nn.softmax(logits), axis=1, output_type=tf.int32), [-1, 1])\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, y), dtype=tf.float32))\n",
    "        tf.summary.scalar('train_accuracy', accuracy)\n",
    "\n",
    "        summaries = tf.summary.merge_all()\n",
    "        test_accuracy_summary = tf.summary.scalar('test_accuracy', accuracy)\n",
    "\n",
    "\n",
    "        graph_init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_save = {\"W_conv1\":W_conv1,\n",
    "                     \"b_conv1\":b_conv1,\n",
    "                     \"W_conv10\":W_conv10,\n",
    "                     \"b_conv10\":b_conv10}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output dir: /tmp/squeezenet/11\n",
      "Iteration: 0\t loss: 2.303\t accuracy: 0.117\t test accuracy: 0.101\n",
      "Iteration: 100\t loss: 2.064\t accuracy: 0.250\t test accuracy: 0.281\n",
      "Iteration: 200\t loss: 1.727\t accuracy: 0.297\t test accuracy: 0.336\n",
      "Iteration: 300\t loss: 1.437\t accuracy: 0.492\t test accuracy: 0.485\n",
      "Iteration: 400\t loss: 1.220\t accuracy: 0.500\t test accuracy: 0.590\n",
      "Iteration: 500\t loss: 0.824\t accuracy: 0.711\t test accuracy: 0.729\n",
      "Iteration: 600\t loss: 0.611\t accuracy: 0.773\t test accuracy: 0.760\n",
      "Iteration: 700\t loss: 0.951\t accuracy: 0.711\t test accuracy: 0.769\n",
      "Iteration: 800\t loss: 0.445\t accuracy: 0.852\t test accuracy: 0.802\n",
      "Iteration: 900\t loss: 0.536\t accuracy: 0.805\t test accuracy: 0.808\n",
      "Iteration: 1000\t loss: 0.641\t accuracy: 0.789\t test accuracy: 0.830\n",
      "Iteration: 1100\t loss: 0.416\t accuracy: 0.891\t test accuracy: 0.840\n",
      "Iteration: 1200\t loss: 0.419\t accuracy: 0.898\t test accuracy: 0.842\n",
      "Iteration: 1300\t loss: 0.531\t accuracy: 0.820\t test accuracy: 0.859\n",
      "Iteration: 1400\t loss: 0.481\t accuracy: 0.859\t test accuracy: 0.882\n",
      "Iteration: 1500\t loss: 0.406\t accuracy: 0.852\t test accuracy: 0.865\n",
      "Iteration: 1600\t loss: 0.392\t accuracy: 0.883\t test accuracy: 0.903\n",
      "Iteration: 1700\t loss: 0.531\t accuracy: 0.836\t test accuracy: 0.903\n",
      "Iteration: 1800\t loss: 0.424\t accuracy: 0.883\t test accuracy: 0.906\n",
      "Iteration: 1900\t loss: 0.162\t accuracy: 0.961\t test accuracy: 0.918\n",
      "Iteration: 2000\t loss: 0.294\t accuracy: 0.898\t test accuracy: 0.918\n",
      "Iteration: 2100\t loss: 0.297\t accuracy: 0.914\t test accuracy: 0.913\n",
      "Iteration: 2200\t loss: 0.381\t accuracy: 0.883\t test accuracy: 0.914\n",
      "Iteration: 2300\t loss: 0.267\t accuracy: 0.883\t test accuracy: 0.912\n",
      "Iteration: 2400\t loss: 0.254\t accuracy: 0.938\t test accuracy: 0.922\n",
      "Iteration: 2500\t loss: 0.203\t accuracy: 0.953\t test accuracy: 0.925\n",
      "Iteration: 2600\t loss: 0.229\t accuracy: 0.945\t test accuracy: 0.927\n",
      "Iteration: 2700\t loss: 0.190\t accuracy: 0.969\t test accuracy: 0.926\n",
      "Iteration: 2800\t loss: 0.139\t accuracy: 0.961\t test accuracy: 0.930\n",
      "Iteration: 2900\t loss: 0.335\t accuracy: 0.906\t test accuracy: 0.932\n",
      "Iteration: 3000\t loss: 0.156\t accuracy: 0.953\t test accuracy: 0.922\n",
      "Iteration: 3100\t loss: 0.264\t accuracy: 0.922\t test accuracy: 0.919\n",
      "Iteration: 3200\t loss: 0.253\t accuracy: 0.938\t test accuracy: 0.924\n",
      "Iteration: 3300\t loss: 0.221\t accuracy: 0.906\t test accuracy: 0.920\n",
      "Iteration: 3400\t loss: 0.198\t accuracy: 0.945\t test accuracy: 0.936\n",
      "Iteration: 3500\t loss: 0.088\t accuracy: 0.977\t test accuracy: 0.923\n",
      "Iteration: 3600\t loss: 0.223\t accuracy: 0.938\t test accuracy: 0.932\n",
      "Iteration: 3700\t loss: 0.268\t accuracy: 0.891\t test accuracy: 0.935\n",
      "Iteration: 3800\t loss: 0.353\t accuracy: 0.898\t test accuracy: 0.927\n",
      "Iteration: 3900\t loss: 0.447\t accuracy: 0.891\t test accuracy: 0.929\n",
      "Iteration: 4000\t loss: 0.175\t accuracy: 0.977\t test accuracy: 0.925\n",
      "Iteration: 4100\t loss: 0.215\t accuracy: 0.953\t test accuracy: 0.921\n",
      "Iteration: 4200\t loss: 0.207\t accuracy: 0.922\t test accuracy: 0.937\n",
      "Iteration: 4300\t loss: 0.193\t accuracy: 0.914\t test accuracy: 0.934\n",
      "Iteration: 4400\t loss: 0.242\t accuracy: 0.922\t test accuracy: 0.926\n",
      "Iteration: 4500\t loss: 0.162\t accuracy: 0.945\t test accuracy: 0.944\n"
     ]
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "\n",
    "n_epochs = 5\n",
    "minibatch_size = 128\n",
    "iterations = 10000 #this get u to 92%, anything more is overfitting\n",
    "import os\n",
    "with tf.Session(graph=without_ABC_graph) as sess:\n",
    "    sess.run(graph_init)\n",
    "    experiment_dir = next_experiment_dir('/tmp/squeezenet')\n",
    "    print(\"Creating output dir:\", experiment_dir)\n",
    "    train_writer = tf.summary.FileWriter(experiment_dir, sess.graph)\n",
    "\n",
    "    for i in range(iterations):\n",
    "            # pick random minibatch\n",
    "        mb_start = np.random.randint(0,num_images_train  - minibatch_size)\n",
    "        mb_end = mb_start + minibatch_size\n",
    "        mb_data = x_train[mb_start:mb_end, :, :, :]\n",
    "        mb_labels = y_train[mb_start:mb_end, :]\n",
    "\n",
    "        feed_dict = {\n",
    "            x: mb_data,\n",
    "            y: mb_labels,\n",
    "            keep_prob: 0.5,\n",
    "            in_training: True,\n",
    "            learning_rate: 0.001  #0.0004\n",
    "        }\n",
    "       # print(i)\n",
    "        collectibles = [loss, accuracy, summaries, optimizer]\n",
    "\n",
    "        loss_val, accuracy_val, s, _ = sess.run(collectibles, feed_dict=feed_dict)\n",
    "\n",
    "        train_writer.add_summary(s, i)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            feed_dict = {\n",
    "                x: x_test,\n",
    "                y: y_test,\n",
    "                keep_prob: 1.0,\n",
    "                in_training: False,\n",
    "                learning_rate: 0.001\n",
    "            }\n",
    "            test_acc, sum_acc = sess.run([accuracy, test_accuracy_summary], feed_dict=feed_dict)\n",
    "            train_writer.add_summary(sum_acc, i)\n",
    "            print('Iteration: {}\\t loss: {:.3f}\\t accuracy: {:.3f}\\t test accuracy: {:.3f}'.format(\n",
    "                i, loss_val, accuracy_val, test_acc))\n",
    "    print(\"Saving\")\n",
    "    for var_name in variables_to_save:\n",
    "        values[var_name] = sess.run(variables_to_save[var_name])\n",
    "    pickle.dump(values, open(\"Smallsave.p\",\"wb\"))\n",
    "\n",
    "        \n",
    "        \n",
    "    # On completion of training, save the variables to be fed to custom model\n",
    "   # for var_name in variables_to_save:   #     values[var_name] = sess.run(variables_to_save[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are using the model w 10,000 samples and 2000 valid so 92% is pretty good. Stop at 5000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 100% accuracy is not an error. It is due to the fact that complete validation set is not being evaluated only part of it is being evaluated and our model got all right answers in that part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the custom model\n",
    "While creating the custom model, we will need to create all the variables ourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a function that returns the required mean and variance for the batchnorm layer. Batchnorm layer requires that mean and variance be calculated of every layer except that of the channels layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "values = pickle.load(open(\"save_slow.p\",\"rb\"))\n",
    "\n",
    "def bn_mean_variance(input_tensor, axis=-1, keep_dims=True):\n",
    "    shape = len(input_tensor.get_shape())\n",
    "    if axis < 0:\n",
    "        axis += shape\n",
    "    dimension_range = range(shape)\n",
    "    return tf.nn.moments(input_tensor, axes=dimension_range[:axis] + dimension_range[axis+1:],\n",
    "                         keep_dims=keep_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_graph = tf.Graph()\n",
    "with custom_graph.as_default():\n",
    "    alphas_training_operations = []\n",
    "    alphas_variables = []\n",
    "    \n",
    "    # Setting configuration\n",
    "    no_filters_conv1 = 2\n",
    "    no_layers_conv1 = 1\n",
    "    no_filters_conv10 = 2\n",
    "    no_layers_conv10 = 1\n",
    "    \n",
    "    pooling_size=(1, 2, 2, 1)\n",
    "\n",
    "    # Inputs\n",
    "    #x = tf.placeholder(dtype=tf.float32)\n",
    "    #x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    x = tf.placeholder(tf.float32,\n",
    "                                     shape=[None, 28, 28, 1],  #use 32, 32, 3 for cifar\n",
    "                                     name='x')\n",
    "    #x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    x_image = x\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    y = tf.placeholder(tf.int32, [None,1],name='y')\n",
    "    in_training = tf.placeholder(tf.bool, shape=())\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=())\n",
    "    # Convolution Layer 1\n",
    "    W_conv1 = tf.Variable(values[\"W_conv1\"], name=\"W_conv1\")\n",
    "    b_conv1 = tf.Variable(values[\"b_conv1\"], name=\"b_conv1\")\n",
    "    #W_conv1 = tf.Variable(tf.contrib.layers.xavier_initializer()([5, 5, 1, 96]),name='W_conv1')\n",
    "    #b_conv1 = tf.Variable(tf.zeros([1, 1, 1, 96]),name='b_conv1')\n",
    "    #X_1 = tf.nn.conv2d(x_image, W_conv1, strides=(1, 2, 2, 1), padding='VALID') + b_conv1\n",
    "    \n",
    "    # Creating new variables\n",
    "    alphas_conv1 = tf.Variable(tf.constant(1., shape=(no_filters_conv1, 1)),\n",
    "                               dtype=tf.float32, name=\"alphas_conv1\")\n",
    "    shift_parameters_conv1 = tf.Variable(tf.constant(0., shape=(no_layers_conv1, 1)),\n",
    "                                         dtype=tf.float32, name=\"shift_parameters_conv1\")\n",
    "    betas_conv1 = tf.Variable(tf.constant(1., shape=(no_layers_conv1, 1)),\n",
    "                              dtype=tf.float32, name=\"betas_conv1\")\n",
    "    # Performing the operations\n",
    "    binary_filters_conv1 = get_binary_filters(W_conv1, no_filters_conv1)\n",
    "    alpha_training_conv1, alpha_loss_conv1 = alpha_training(tf.stop_gradient(W_conv1, \"no_gradient_W_conv1\"),\n",
    "                                                            tf.stop_gradient(binary_filters_conv1,\n",
    "                                                                             \"no_gradient_binary_filters_conv1\"),\n",
    "                                                            alphas_conv1, no_filters_conv1)\n",
    "    conv1 = ABC(binary_filters_conv1, tf.stop_gradient(alphas_conv1), shift_parameters_conv1,\n",
    "                betas_conv1, b_conv1, padding=\"SAME\")(x_image)\n",
    "    # Saving the alphas training operation and the variable\n",
    "    alphas_training_operations.append(alpha_training_conv1)\n",
    "    alphas_variables.append(alphas_conv1)\n",
    "    \n",
    "    \n",
    "    # Other layers\n",
    "   # A_1 = tf.nn.relu(conv1)\n",
    "        #maxpool\n",
    "   # maxpool_1 = tf.nn.max_pool(A_1, ksize=pooling_size, strides=(1, 2, 2, 1), padding='VALID', name='maxpool_1')\n",
    "    \n",
    "    maxpool_1 = tf.nn.max_pool(conv1, ksize=pooling_size, strides=(1, 2, 2, 1), padding='VALID', name='maxpool_1')\n",
    "    ##batch Norm\n",
    "    mean_conv1, variance_conv1 = bn_mean_variance(maxpool_1)\n",
    "    bn_gamma_conv1 = tf.Variable(tf.ones(shape=(96,), dtype=tf.float32), name=\"bn_gamma_conv1\")\n",
    "    bn_beta_conv1 = tf.Variable(tf.zeros(shape=(96,), dtype=tf.float32), name=\"bn_beta_conv1\")\n",
    "    bn_conv1 = tf.nn.batch_normalization(maxpool_1, mean_conv1, variance_conv1,\n",
    "                                         bn_beta_conv1, bn_gamma_conv1, 0.001)\n",
    "    \n",
    "    A_1 = tf.nn.relu(bn_conv1)\n",
    "    \n",
    "        # layer 3-5 - fire modules\n",
    "   # fire_2 = fire_module(maxpool_1, 16, 64, 64, layer_num=2)\n",
    "    fire_2 = fire_module(A_1, 16, 64, 64, layer_num=2)\n",
    "\n",
    "    fire_3 = fire_module(fire_2, 16, 64, 64, layer_num=3)\n",
    "    fire_4 = fire_module(fire_3, 32, 128, 128, layer_num=4)\n",
    "\n",
    "        # layer 6 - maxpool\n",
    "    maxpool_4 = tf.nn.max_pool(fire_4, ksize=pooling_size, strides=(1, 2, 2, 1), padding='VALID', name='maxpool_4')\n",
    "\n",
    "        # layer 7-10 - fire modules\n",
    "    fire_5 = fire_module(maxpool_4, 32, 128, 128, layer_num=5)\n",
    "    fire_6 = fire_module(fire_5, 48, 192, 192, layer_num=6)         \n",
    "    fire_7 = fire_module(fire_6, 48, 192, 192, layer_num=7)\n",
    "    fire_8 = fire_module(fire_7, 64, 256, 256, layer_num=8)\n",
    "\n",
    "        # layer 11 - maxpool\n",
    "    maxpool_8 = tf.nn.max_pool(fire_8, ksize=pooling_size, strides=(1, 2, 2, 1), padding='VALID', name='maxpool_8')\n",
    "\n",
    "        # layer 12 - fire 9 + dropout\n",
    "    fire_9 = fire_module(maxpool_8, 64, 256, 256, layer_num=9)\n",
    "\n",
    "    dropout_9 = tf.cond(in_training,\n",
    "                        lambda: tf.nn.dropout(fire_9, keep_prob=0.5),\n",
    "                        lambda: fire_9)\n",
    "\n",
    "    input_shape = dropout_9.get_shape().as_list()[3]\n",
    "    # BatchNorm \n",
    "\n",
    "    # Convolution Layer 10\n",
    "    W_conv10 = tf.Variable(values[\"W_conv10\"], name=\"W_conv10\")\n",
    "    b_conv10 = tf.Variable(values[\"b_conv10\"], name=\"b_conv10\")\n",
    "    \n",
    "    # Creating new variables\n",
    "    alphas_conv10 = tf.Variable(tf.constant(1., shape=(no_filters_conv10, 1)),\n",
    "                               dtype=tf.float32, name=\"alphas_conv10\")\n",
    "    shift_parameters_conv10 = tf.Variable(tf.constant(0., shape=(no_layers_conv10, 1)),\n",
    "                                         dtype=tf.float32, name=\"shift_parameters_conv10\")\n",
    "    betas_conv10 = tf.Variable(tf.constant(1., shape=(no_layers_conv10, 1)),\n",
    "                              dtype=tf.float32, name=\"betas_conv10\")\n",
    "    \n",
    "    # Performing the operations\n",
    "    binary_filters_conv10 = get_binary_filters(W_conv10, no_filters_conv10)\n",
    "    alpha_training_conv10, alpha_loss_conv10 = alpha_training(tf.stop_gradient(W_conv10, \"no_gradient_W_conv10\"),\n",
    "                                                            tf.stop_gradient(binary_filters_conv10,\n",
    "                                                                             \"no_gradient_binary_filters_conv10\"),\n",
    "                                                            alphas_conv10, no_filters_conv10)\n",
    "    conv10 = ABC(binary_filters_conv10, tf.stop_gradient(alphas_conv10), shift_parameters_conv10,\n",
    "                betas_conv10, b_conv10, padding=\"SAME\")(dropout_9) #connect to dropout 9\n",
    "    \n",
    "    # Saving the alphas training operation and the variable\n",
    "    alphas_training_operations.append(alpha_training_conv10)\n",
    "    alphas_variables.append(alphas_conv10)\n",
    "    \n",
    "    # Other layers\n",
    " #   A_conv_10 = tf.nn.relu(conv10)\n",
    "    # BatchNorm\n",
    "   # _, H_last, W_last, _ = A_conv_10.get_shape().as_list()\n",
    "    _, H_last, W_last, _ = conv10.get_shape().as_list()\n",
    "\n",
    "    #pooled = tf.nn.avg_pool(A_conv_10, ksize=(1, H_last, W_last, 1), strides=(1, 1, 1, 1), padding='VALID')\n",
    "    pooled = tf.nn.avg_pool(conv10, ksize=(1, H_last, W_last, 1), strides=(1, 1, 1, 1), padding='VALID')\n",
    "    #batchnorm\n",
    "    mean_conv10, variance_conv10 = bn_mean_variance(pooled)\n",
    "    bn_gamma_conv10 = tf.Variable(tf.ones(shape=(10,), dtype=tf.float32), name=\"bn_gamma_conv10\")\n",
    "    bn_beta_conv10 = tf.Variable(tf.zeros(shape=(10,), dtype=tf.float32), name=\"bn_beta_conv10\")\n",
    "    bn_conv10 = tf.nn.batch_normalization(pooled, mean_conv10, variance_conv10,\n",
    "                                         bn_beta_conv10, bn_gamma_conv10, 0.001)\n",
    "    A_conv_10 = tf.nn.relu(bn_conv10)\n",
    "    logits = tf.squeeze(A_conv_10, axis=[1, 2])\n",
    "\n",
    "  #  logits = tf.squeeze(pooled, axis=[1, 2])\n",
    "\n",
    "    one_hot_labels = tf.one_hot(y, 10, name='one_hot_encoding')\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=one_hot_labels, logits=logits))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # accuracy\n",
    "    predictions = tf.reshape(tf.argmax(tf.nn.softmax(logits), axis=1, output_type=tf.int32), [-1, 1])\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, y), dtype=tf.float32))\n",
    "    tf.summary.scalar('train_accuracy', accuracy)\n",
    "\n",
    "    summaries = tf.summary.merge_all()\n",
    "    test_accuracy_summary = tf.summary.scalar('test_accuracy', accuracy)\n",
    "\n",
    "    #init\n",
    "    graph_init = tf.global_variables_initializer()\n",
    "    alphas_init = tf.variables_initializer(alphas_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the dictionary of variables to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining variables to save. These will be fed to our custom layer\n",
    "variables_to_save = {\"W_conv1\": W_conv1,\n",
    "                     \"b_conv1\": b_conv1,\n",
    "                     \"alphas_conv1\": alphas_conv1,\n",
    "                     \"betas_conv1\": betas_conv1,\n",
    "                     \"shift_parameters_conv1\": shift_parameters_conv1,\n",
    "                     \"W_conv10\": W_conv10,\n",
    "                     \"b_conv10\": b_conv10,\n",
    "                     \"alphas_conv10\": alphas_conv10,\n",
    "                     \"betas_conv10\": betas_conv10,\n",
    "                     \"shift_parameters_conv10\": shift_parameters_conv10,\n",
    "                    }\n",
    "values = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello?\n",
      "Iteration: 0\t loss: 2.303\t accuracy: 0.117\t test accuracy: 0.100\n",
      "Iteration: 100\t loss: 2.301\t accuracy: 0.148\t test accuracy: 0.095\n",
      "Iteration: 200\t loss: 2.302\t accuracy: 0.125\t test accuracy: 0.095\n",
      "Iteration: 300\t loss: 2.300\t accuracy: 0.141\t test accuracy: 0.095\n",
      "Iteration: 400\t loss: 2.299\t accuracy: 0.148\t test accuracy: 0.095\n",
      "Iteration: 500\t loss: 2.304\t accuracy: 0.086\t test accuracy: 0.095\n",
      "Iteration: 600\t loss: 2.303\t accuracy: 0.094\t test accuracy: 0.095\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-7671ccdca9b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'n_epochs = 5\\nminibatch_size = 128\\nalpha_training_epochs = 100\\niterations = 2000 #this get u to 92%, anything more is overfitting\\nimport os\\nprint(\"hello?\")\\nwith tf.Session(graph=custom_graph) as sess:\\n    sess.run(graph_init)\\n  #  experiment_dir = next_experiment_dir(\\'/tmp/squeezenet\\')\\n  #  print(\"Creating output dir:\", experiment_dir)\\n  #  train_writer = tf.summary.FileWriter(experiment_dir, sess.graph)\\n\\n    for i in range(iterations):\\n        # Training alphas\\n        sess.run(alphas_init)\\n        for alpha_training_op in alphas_training_operations:\\n            for alpha_epoch in range(alpha_training_epochs):\\n                sess.run(alpha_training_op)\\n        # pick random minibatch\\n        mb_start = np.random.randint(0,num_images_train  - minibatch_size)\\n        mb_end = mb_start + minibatch_size\\n        mb_data = x_train[mb_start:mb_end, :, :, :]\\n        mb_labels = y_train[mb_start:mb_end, :]\\n\\n        feed_dict = {\\n            x: mb_data,\\n            y: mb_labels,\\n            keep_prob: 0.5,\\n            in_training: True,\\n            learning_rate: 0.0004\\n        }\\n\\n        collectibles = [loss, accuracy, summaries, optimizer]\\n\\n        loss_val, accuracy_val, s, _ = sess.run(collectibles, feed_dict=feed_dict)\\n\\n     #   train_writer.add_summary(s, i)\\n\\n        if i % 100 == 0: #after 100 iters\\n            # Training alphas\\n            sess.run(alphas_init)\\n            for alpha_training_op in alphas_training_operations:\\n                for alpha_epoch in range(alpha_training_epochs):\\n                    sess.run(alpha_training_op)\\n            feed_dict = {\\n                x: x_test,\\n                y: y_test,\\n                keep_prob: 1.0,\\n                in_training: False,\\n                learning_rate: 0.0004 \\n            }\\n            test_acc, sum_acc = sess.run([accuracy, test_accuracy_summary], feed_dict=feed_dict)\\n      #      train_writer.add_summary(sum_acc, i)\\n            print(\\'Iteration: {}\\\\t loss: {:.3f}\\\\t accuracy: {:.3f}\\\\t test accuracy: {:.3f}\\'.format(\\n                i, loss_val, accuracy_val, test_acc))\\n    print(\"Saving\")\\n    for var_name in variables_to_save:\\n        values[var_name] = sess.run(variables_to_save[var_name])\\n    pickle.dump(values, open(\"save2.p\",\"wb\"))\\n\\n\\n#n_epochs = 5\\n#batch_size = 32\\n#alpha_training_epochs = 200\\n        \\n#with tf.Session(graph=custom_graph) as sess:\\n#    sess.run(graph_init)\\n#    for epoch in range(n_epochs):\\n#        for iteration in range(1, 200 + 1):\\n            # Training alphas\\n#            sess.run(alphas_init)\\n#            for alpha_training_op in alphas_training_operations:\\n#                for alpha_epoch in range(alpha_training_epochs):\\n#                    sess.run(alpha_training_op)\\n            \\n#            batch = mnist.train.next_batch(50)\\n            \\n            # Run operation and calculate loss\\n#            _, loss_train = sess.run([train_step, cross_entropy],\\n#                                     feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\\n#            print(\"\\\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\\n#                      iteration, 200,\\n#                      iteration * 100 / 200,\\n#                      loss_train),\\n#                  end=\"\")\\n\\n        # At the end of each epoch,\\n        # measure the validation loss and accuracy:\\n        \\n        # Training alphas\\n#        sess.run(alphas_init)\\n#        for alpha_training_op in alphas_training_operations:\\n#            for alpha_epoch in range(alpha_training_epochs):\\n#                sess.run(alpha_training_op)\\n                    \\n#        loss_vals = []\\n#        acc_vals = []\\n#        for iteration in range(1, 200 + 1):            \\n#            X_batch, y_batch = mnist.validation.next_batch(batch_size)\\n#            acc_val, loss_val = sess.run([accuracy, cross_entropy],\\n#                                     feed_dict={x: batch[0], y: batch[1], keep_prob: 1.0})\\n#            loss_vals.append(loss_val)\\n#            acc_vals.append(acc_val)\\n#            print(\"\\\\rEvaluating the model: {}/{} ({:.1f}%)\".format(iteration, 200,\\n#                iteration * 100 / 200),\\n#                  end=\" \" * 10)\\n#        loss_val = np.mean(loss_vals)\\n#        acc_val = np.mean(acc_vals)\\n#        print(\"\\\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}\".format(\\n#            epoch + 1, acc_val * 100, loss_val))\\n        \\n    # On completion of training, save the variables to be fed to custom model\\n#    for var_name in variables_to_save:\\n#        values[var_name] = sess.run(variables_to_save[var_name])'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 908\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    909\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1143\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1144\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1324\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1325\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1328\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1315\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1421\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1422\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1423\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_epochs = 5\n",
    "minibatch_size = 128\n",
    "alpha_training_epochs = 100\n",
    "iterations = 2000 #this get u to 92%, anything more is overfitting\n",
    "import os\n",
    "print(\"hello?\")\n",
    "with tf.Session(graph=custom_graph) as sess:\n",
    "    sess.run(graph_init)\n",
    "  #  experiment_dir = next_experiment_dir('/tmp/squeezenet')\n",
    "  #  print(\"Creating output dir:\", experiment_dir)\n",
    "  #  train_writer = tf.summary.FileWriter(experiment_dir, sess.graph)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Training alphas\n",
    "        sess.run(alphas_init)\n",
    "        for alpha_training_op in alphas_training_operations:\n",
    "            for alpha_epoch in range(alpha_training_epochs):\n",
    "                sess.run(alpha_training_op)\n",
    "        # pick random minibatch\n",
    "        mb_start = np.random.randint(0,num_images_train  - minibatch_size)\n",
    "        mb_end = mb_start + minibatch_size\n",
    "        mb_data = x_train[mb_start:mb_end, :, :, :]\n",
    "        mb_labels = y_train[mb_start:mb_end, :]\n",
    "\n",
    "        feed_dict = {\n",
    "            x: mb_data,\n",
    "            y: mb_labels,\n",
    "            keep_prob: 0.5,\n",
    "            in_training: True,\n",
    "            learning_rate: 0.0004\n",
    "        }\n",
    "\n",
    "        collectibles = [loss, accuracy, summaries, optimizer]\n",
    "\n",
    "        loss_val, accuracy_val, s, _ = sess.run(collectibles, feed_dict=feed_dict)\n",
    "\n",
    "     #   train_writer.add_summary(s, i)\n",
    "\n",
    "        if i % 100 == 0: #after 100 iters\n",
    "            # Training alphas\n",
    "            sess.run(alphas_init)\n",
    "            for alpha_training_op in alphas_training_operations:\n",
    "                for alpha_epoch in range(alpha_training_epochs):\n",
    "                    sess.run(alpha_training_op)\n",
    "            feed_dict = {\n",
    "                x: x_test,\n",
    "                y: y_test,\n",
    "                keep_prob: 1.0,\n",
    "                in_training: False,\n",
    "                learning_rate: 0.0004 \n",
    "            }\n",
    "            test_acc, sum_acc = sess.run([accuracy, test_accuracy_summary], feed_dict=feed_dict)\n",
    "      #      train_writer.add_summary(sum_acc, i)\n",
    "            print('Iteration: {}\\t loss: {:.3f}\\t accuracy: {:.3f}\\t test accuracy: {:.3f}'.format(\n",
    "                i, loss_val, accuracy_val, test_acc))\n",
    "    print(\"Saving\")\n",
    "    for var_name in variables_to_save:\n",
    "        values[var_name] = sess.run(variables_to_save[var_name])\n",
    "    pickle.dump(values, open(\"save2.p\",\"wb\"))\n",
    "\n",
    "\n",
    "#n_epochs = 5\n",
    "#batch_size = 32\n",
    "#alpha_training_epochs = 200\n",
    "        \n",
    "#with tf.Session(graph=custom_graph) as sess:\n",
    "#    sess.run(graph_init)\n",
    "#    for epoch in range(n_epochs):\n",
    "#        for iteration in range(1, 200 + 1):\n",
    "            # Training alphas\n",
    "#            sess.run(alphas_init)\n",
    "#            for alpha_training_op in alphas_training_operations:\n",
    "#                for alpha_epoch in range(alpha_training_epochs):\n",
    "#                    sess.run(alpha_training_op)\n",
    "            \n",
    "#            batch = mnist.train.next_batch(50)\n",
    "            \n",
    "            # Run operation and calculate loss\n",
    "#            _, loss_train = sess.run([train_step, cross_entropy],\n",
    "#                                     feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "#            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "#                      iteration, 200,\n",
    "#                      iteration * 100 / 200,\n",
    "#                      loss_train),\n",
    "#                  end=\"\")\n",
    "\n",
    "        # At the end of each epoch,\n",
    "        # measure the validation loss and accuracy:\n",
    "        \n",
    "        # Training alphas\n",
    "#        sess.run(alphas_init)\n",
    "#        for alpha_training_op in alphas_training_operations:\n",
    "#            for alpha_epoch in range(alpha_training_epochs):\n",
    "#                sess.run(alpha_training_op)\n",
    "                    \n",
    "#        loss_vals = []\n",
    "#        acc_vals = []\n",
    "#        for iteration in range(1, 200 + 1):            \n",
    "#            X_batch, y_batch = mnist.validation.next_batch(batch_size)\n",
    "#            acc_val, loss_val = sess.run([accuracy, cross_entropy],\n",
    "#                                     feed_dict={x: batch[0], y: batch[1], keep_prob: 1.0})\n",
    "#            loss_vals.append(loss_val)\n",
    "#            acc_vals.append(acc_val)\n",
    "#            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(iteration, 200,\n",
    "#                iteration * 100 / 200),\n",
    "#                  end=\" \" * 10)\n",
    "#        loss_val = np.mean(loss_vals)\n",
    "#        acc_val = np.mean(acc_vals)\n",
    "#        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
    "#            epoch + 1, acc_val * 100, loss_val))\n",
    "        \n",
    "    # On completion of training, save the variables to be fed to custom model\n",
    "#    for var_name in variables_to_save:\n",
    "#        values[var_name] = sess.run(variables_to_save[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, only the required variables can be saved for inference time. Using the **W_conv1** and **W_conv2**, values for binary filters and alphas can be calculated and those can be used along with **shift_parameters** and **betas** to create ABC layer for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure inference testing\n",
    "OK! Let's extract the binary filters and alphas and throw away the weights and test our network. This will ensure that we do not have any bug in the implementation of the ABC layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating graphs for alphas calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1_cal_graph = tf.Graph()\n",
    "with alpha1_cal_graph.as_default():\n",
    "    alphas1 = tf.Variable(tf.zeros(shape=(no_filters_conv1, 1), dtype=tf.float32))\n",
    "    conv_filters1 = tf.placeholder(dtype=tf.float32, shape=(5, 5, 1, 32))\n",
    "    bin_filters1 = get_binary_filters(convolution_filters=conv_filters1,\n",
    "                                     no_filters=no_filters_conv1)\n",
    "    alpha_training_op1, alpha_training_loss1 = alpha_training(conv_filters1, bin_filters1,\n",
    "                                                            alphas1, no_filters_conv1)\n",
    "    al_init1 = tf.global_variables_initializer()\n",
    "    \n",
    "alpha2_cal_graph = tf.Graph()\n",
    "with alpha2_cal_graph.as_default():\n",
    "    alphas2 = tf.Variable(tf.zeros(shape=(no_filters_conv2, 1), dtype=tf.float32))\n",
    "    conv_filters2 = tf.placeholder(dtype=tf.float32, shape=(5, 5, 32, 64))\n",
    "    bin_filters2 = get_binary_filters(convolution_filters=conv_filters2,\n",
    "                                     no_filters=no_filters_conv2)\n",
    "    alpha_training_op2, alpha_training_loss2 = alpha_training(conv_filters2, bin_filters2,\n",
    "                                                            alphas2, no_filters_conv2)\n",
    "    al_init2 = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating alphas and binary filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=alpha1_cal_graph) as sess:\n",
    "    al_init1.run()\n",
    "    for epoch in range(200):\n",
    "        sess.run(alpha_training_op1, feed_dict={conv_filters1: values[\"W_conv1\"]})\n",
    "    cal_bin_filters, cal_alphas = sess.run([bin_filters1, alphas1], feed_dict={conv_filters1: values[\"W_conv1\"]})\n",
    "    values[\"binary_filters_conv1\"] = cal_bin_filters\n",
    "    values[\"alphas_conv1\"] = cal_alphas\n",
    "\n",
    "with tf.Session(graph=alpha2_cal_graph) as sess:\n",
    "    al_init2.run()\n",
    "    for epoch in range(200):\n",
    "        sess.run(alpha_training_op2, feed_dict={conv_filters2: values[\"W_conv2\"]})\n",
    "    cal_bin_filters, cal_alphas = sess.run([bin_filters2, alphas2], feed_dict={conv_filters2: values[\"W_conv2\"]})\n",
    "    values[\"binary_filters_conv2\"] = cal_bin_filters\n",
    "    values[\"alphas_conv2\"] = cal_alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building inference model\n",
    "Now, we have all our variables, let's build an inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_graph = tf.Graph()\n",
    "with inference_graph.as_default():\n",
    "    # Setting configuration\n",
    "    no_filters_conv1 = 5\n",
    "    no_layers_conv1 = 5\n",
    "    no_filters_conv2 = 5\n",
    "    no_layers_conv2 = 5\n",
    "    \n",
    "    # Inputs\n",
    "    x = tf.placeholder(dtype=tf.float32)\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    \n",
    "    # Convolution Layer 1\n",
    "    b_conv1 = tf.convert_to_tensor(values[\"b_conv1\"], dtype=tf.float32, name=\"b_conv1\")\n",
    "    alphas_conv1 = tf.convert_to_tensor(values[\"alphas_conv1\"],\n",
    "                                        dtype=tf.float32, name=\"alphas_conv1\")\n",
    "    shift_parameters_conv1 = tf.convert_to_tensor(values[\"shift_parameters_conv1\"],\n",
    "                                                  dtype=tf.float32, name=\"shift_parameters_conv1\")\n",
    "    betas_conv1 = tf.convert_to_tensor(values[\"betas_conv1\"],\n",
    "                                       dtype=tf.float32, name=\"betas_conv1\")\n",
    "    # Performing the operations\n",
    "    binary_filters_conv1 = tf.convert_to_tensor(values[\"binary_filters_conv1\"], dtype=tf.float32,\n",
    "                                                name=\"binary_filters_conv1\")\n",
    "    conv1 = ABC(binary_filters_conv1, tf.stop_gradient(alphas_conv1), shift_parameters_conv1,\n",
    "                betas_conv1, b_conv1, padding=\"SAME\")(x_image)\n",
    "    # Other layers\n",
    "    pool1 = max_pool_2x2(conv1)\n",
    "    # batch norm parameters\n",
    "    mean_conv1, variance_conv1 = bn_mean_variance(pool1)\n",
    "    bn_gamma_conv1 = tf.convert_to_tensor(values[\"bn_gamma_conv1\"], dtype=tf.float32,\n",
    "                                          name=\"bn_gamma_conv1\")\n",
    "    bn_beta_conv1 = tf.convert_to_tensor(values[\"bn_beta_conv1\"], dtype=tf.float32,\n",
    "                                         name=\"bn_beta_conv1\")\n",
    "    bn_conv1 = tf.nn.batch_normalization(pool1, mean_conv1, variance_conv1,\n",
    "                                         bn_beta_conv1, bn_gamma_conv1, 0.001)\n",
    "    h_conv1 = tf.nn.relu(bn_conv1)\n",
    "\n",
    "    # Convolution Layer 2\n",
    "    b_conv2 = tf.convert_to_tensor(values[\"b_conv2\"], dtype=tf.float32, name=\"b_conv2\")\n",
    "    alphas_conv2 = tf.convert_to_tensor(values[\"alphas_conv2\"],\n",
    "                                        dtype=tf.float32, name=\"alphas_conv2\")\n",
    "    shift_parameters_conv2 = tf.convert_to_tensor(values[\"shift_parameters_conv2\"],\n",
    "                                                  dtype=tf.float32, name=\"shift_parameters_conv2\")\n",
    "    betas_conv2 = tf.convert_to_tensor(values[\"betas_conv2\"],\n",
    "                                       dtype=tf.float32, name=\"betas_conv2\")\n",
    "    # Performing the operations\n",
    "    binary_filters_conv2 = tf.convert_to_tensor(values[\"binary_filters_conv2\"], dtype=tf.float32,\n",
    "                                                name=\"binary_filters_conv2\")\n",
    "    conv2 = ABC(binary_filters_conv2, tf.stop_gradient(alphas_conv2), shift_parameters_conv2,\n",
    "                betas_conv2, b_conv2, padding=\"SAME\")(h_conv1)\n",
    "    # Other layers\n",
    "    pool2 = max_pool_2x2(conv2)\n",
    "    # batch norm parameters\n",
    "    mean_conv2, variance_conv2 = bn_mean_variance(pool2)\n",
    "    bn_gamma_conv2 = tf.convert_to_tensor(values[\"bn_gamma_conv2\"], dtype=tf.float32,\n",
    "                                          name=\"bn_gamma_conv2\")\n",
    "    bn_beta_conv2 = tf.convert_to_tensor(values[\"bn_beta_conv2\"], dtype=tf.float32,\n",
    "                                         name=\"bn_beta_conv2\")\n",
    "    bn_conv2 = tf.nn.batch_normalization(pool2, mean_conv2, variance_conv2,\n",
    "                                         bn_beta_conv2, bn_gamma_conv2, 0.001)\n",
    "    h_conv2 = tf.nn.relu(bn_conv2)\n",
    "\n",
    "    # Flat the conv2 output\n",
    "    h_conv2_flat = tf.reshape(h_conv2, shape=(-1, 7*7*64))\n",
    "\n",
    "    # Dense layer1\n",
    "    W_fc1 = tf.convert_to_tensor(values[\"W_fc1\"], dtype=tf.float32)\n",
    "    b_fc1 = tf.convert_to_tensor(values[\"b_fc1\"], dtype=tf.float32)\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Dropout\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # Output layer\n",
    "    W_fc2 = tf.convert_to_tensor(values[\"W_fc2\"], dtype=tf.float32)\n",
    "    b_fc2 = tf.convert_to_tensor(values[\"b_fc2\"], dtype=tf.float32)\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    \n",
    "    # Labels\n",
    "    y = tf.placeholder(tf.int32, [None])\n",
    "    y_ = tf.one_hot(y, 10)\n",
    "    \n",
    "    # Defining optimizer and loss\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with tf.Session(graph=inference_graph) as sess:\n",
    "    loss_vals = []\n",
    "    acc_vals = []\n",
    "    for iteration in range(1, 500 + 1):            \n",
    "        X_batch, y_batch = mnist.validation.next_batch(batch_size)\n",
    "        acc_val, loss_val = sess.run([accuracy, cross_entropy],\n",
    "                                 feed_dict={x: batch[0], y: batch[1], keep_prob: 1.0})\n",
    "        loss_vals.append(loss_val)\n",
    "        acc_vals.append(acc_val)\n",
    "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(iteration, 500,\n",
    "            iteration * 100 / 500),\n",
    "              end=\" \" * 10)\n",
    "    loss_val = np.mean(loss_vals)\n",
    "    acc_val = np.mean(acc_vals)\n",
    "    print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
    "        epoch + 1, acc_val * 100, loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
